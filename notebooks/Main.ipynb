{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Breast Cancer Mass Detection with EfficientDet",
   "id": "e2a739b434ef3c77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Necessary Imports"
   ],
   "id": "f02b79ceb2bfb3ef"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dataset Operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File Operations\n",
    "import os\n",
    "\n",
    "# Model Operations\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "# Others\n",
    "import glob\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "############  TEST  ############\n",
    "print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())"
   ],
   "id": "de2b095e161bdb2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "id": "c2525199f227a068"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the Dataset "
   ],
   "id": "2bcc231dac23b870"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def resize_image(image_path, target_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    This function loads an image from the given file path, pads it to a square shape with zeros, \n",
    "    and then resizes it to the target size.\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): The file path to the image.\n",
    "    target_size (tuple): The target size in the format (width, height).\n",
    "\n",
    "    Returns:\n",
    "    np.array: The resized image.\n",
    "    \"\"\"\n",
    "    # Load the image from the file path\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Get the original size of the image\n",
    "    original_dimensions = (image.shape[1], image.shape[0])\n",
    "    \n",
    "    # Determine the longer side to use as the size for both dimensions\n",
    "    max_side = max(image.shape[:2])\n",
    "    \n",
    "    # Calculate padding for the top, bottom, left, and right\n",
    "    top = (max_side - image.shape[0]) // 2\n",
    "    bottom = max_side - image.shape[0] - top\n",
    "    left = (max_side - image.shape[1]) // 2\n",
    "    right = max_side - image.shape[1] - left\n",
    "    \n",
    "    # Apply padding to make the image square\n",
    "    image_padded = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "    \n",
    "    # Resize the padded image to the target size\n",
    "    resized_image = cv2.resize(image_padded, target_size)\n",
    "    \n",
    "    return resized_image"
   ],
   "id": "d78ba7ad483f3f77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def adjust_and_scale_bounding_box(bbox, padded_size, new_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Scale the bounding box coordinates based on the resizing of the image.\n",
    "\n",
    "    Parameters:\n",
    "    bbox (list): The bounding box coordinates in the format [x1, y1, x2, y2, x3, y3, x4, y4].\n",
    "    padded_size (tuple): The padded size of the original image, in the format (width, height).\n",
    "    new_size (tuple): The new size of the image after resizing, in the format (width, height).\n",
    "\n",
    "    Returns:\n",
    "    list: The scaled bounding box coordinates in the format [y1(ymin), x1(xmin), y3(ymax), x3(xmax)].\n",
    "    \"\"\"\n",
    "    padded_width, padded_height = padded_size\n",
    "    new_width, new_height = new_size\n",
    "    \n",
    "    # Calculate scaling factors\n",
    "    scale_x = new_width / padded_width\n",
    "    scale_y = new_height / padded_height\n",
    "    \n",
    "    # Scale the coordinates\n",
    "    scaled_bbox = []\n",
    "    for i in range(0, len(bbox), 2):\n",
    "        x = bbox[i]\n",
    "        y = bbox[i + 1]\n",
    "        scaled_bbox.append(x * scale_x)\n",
    "        scaled_bbox.append(y * scale_y)\n",
    "    \n",
    "    # Calculate the offsets\n",
    "    x_offset = new_width / 2\n",
    "    y_offset = new_height / 2\n",
    "\n",
    "    # Adjust the coordinates\n",
    "    adjusted_bounding_box = []\n",
    "    for i in range(len(scaled_bbox)):\n",
    "        if(i % 2 == 0): # x-coordinates\n",
    "            adjusted_bounding_box.append(scaled_bbox[i] + x_offset)\n",
    "        else: # y-coordinates\n",
    "            adjusted_bounding_box.append(scaled_bbox[i] + y_offset)\n",
    "\n",
    "    # New bounding box format: [ymin, xmin, ymax, xmax]\n",
    "    ymin = min(adjusted_bounding_box[1], adjusted_bounding_box[3], adjusted_bounding_box[5], adjusted_bounding_box[7])\n",
    "    xmin = min(adjusted_bounding_box[0], adjusted_bounding_box[2], adjusted_bounding_box[4], adjusted_bounding_box[6])\n",
    "    ymax = max(adjusted_bounding_box[1], adjusted_bounding_box[3], adjusted_bounding_box[5], adjusted_bounding_box[7])\n",
    "    xmax = max(adjusted_bounding_box[0], adjusted_bounding_box[2], adjusted_bounding_box[4], adjusted_bounding_box[6])\n",
    "    \n",
    "    return [ymin, xmin, ymax, xmax]"
   ],
   "id": "efb717da18bfc771",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_bounding_box_coordinates(bounding_box_list):\n",
    "    new_list = []\n",
    "    for bounding_box in bounding_box_list:\n",
    "        x = float(bounding_box.split('/')[0])\n",
    "        y = float(bounding_box.split('/')[1])\n",
    "        new_list.append(x)\n",
    "        new_list.append(y)\n",
    "    return new_list"
   ],
   "id": "66872bb0a7fb50bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_example(image, labels, bounding_boxes_list):\n",
    "    \"\"\"\n",
    "    Create a TFRecord from the image, labels, and bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "    image (np.array): The image to be stored in the TFRecord.\n",
    "    labels (list): A list of labels corresponding to each bounding box.\n",
    "    bounding_boxes_list (list): A list of bounding box coordinates, each in the format [ymin, xmin, ymax, xmax].\n",
    "\n",
    "    Returns:\n",
    "    tf.train.Example: The TFRecord example.\n",
    "    \"\"\"\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Encode the image\n",
    "    encoded_image = tf.io.encode_jpeg(image).numpy()\n",
    "\n",
    "    # Initialize lists to store normalized coordinates\n",
    "    ymins = []\n",
    "    xmins = []\n",
    "    ymaxs = []\n",
    "    xmaxs = []\n",
    "\n",
    "    for bbox in bounding_boxes_list:\n",
    "        ymin, xmin, ymax, xmax = bbox\n",
    "        ymins.append(ymin / height)\n",
    "        xmins.append(xmin / width)\n",
    "        ymaxs.append(ymax / height)\n",
    "        xmaxs.append(xmax / width)\n",
    "\n",
    "    # Create the feature dictionary\n",
    "    feature = {\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_image])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[b'jpeg'])),\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
    "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
    "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
    "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
    "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=labels))\n",
    "    }\n",
    "\n",
    "    # Return a tf.train.Example\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))                                            "
   ],
   "id": "da9e555fd1d562d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_tfrecord(image, label, bounding_boxes_list, output_path):\n",
    "    \"\"\"\n",
    "    Create a TFRecord file for a single image.\n",
    "\n",
    "    Parameters:\n",
    "    image (np.array): The image to be stored in the TFRecord.\n",
    "    label (int): The label of the image.\n",
    "    bounding_boxes_list (list): A list of bounding box coordinates, each in the format [ymin, xmin, ymax, xmax].\n",
    "    output_path (str): The output path for the TFRecord file.\n",
    "    \"\"\"\n",
    "    tf_example = create_example(image, label, bounding_boxes_list)\n",
    "    \n",
    "    with tf.io.TFRecordWriter(output_path) as writer:\n",
    "        writer.write(tf_example.SerializeToString())"
   ],
   "id": "d8b5a77fadd7d8dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the csv file",
   "id": "a2b2b115be722d93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def convert_images_to_tfrecords(image_root, tfrecord_root, bbox_data_csv_path):\n",
    "    \"\"\"\n",
    "    Converts images and bounding box data into TFRecord format.\n",
    "\n",
    "    Parameters:\n",
    "    - image_root: The root directory containing the images.\n",
    "    - tfrecord_root: The directory where the TFRecord files will be saved.\n",
    "    - bbox_data_csv_path: The path to the CSV file containing bounding box data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    bbox_data = pd.read_csv(bbox_data_csv_path, delimiter=\";\")\n",
    "\n",
    "    bbox_data = bbox_data[bbox_data['label_name'] == \"mass\"]\n",
    "\n",
    "    # Add a new column for the modified file name (to .jpeg)\n",
    "    bbox_data['modified_file_name'] = bbox_data['file_name'].apply(lambda x: os.path.splitext(x)[0] + \".jpeg\")\n",
    "\n",
    "    # Group by 'category', 'patient_id', and 'modified_file_name'\n",
    "    grouped = bbox_data.groupby(['category', 'patient_id', 'modified_file_name'])\n",
    "\n",
    "    for (category, patient_id, modified_file_name), group in grouped:\n",
    "        # Construct the file path for the image\n",
    "        file_path_train = os.path.join(image_root, \"train\", category, str(patient_id), modified_file_name)\n",
    "        file_path_test = os.path.join(image_root, \"test\", category, str(patient_id), modified_file_name)\n",
    "\n",
    "        # Determine the correct file path\n",
    "        if os.path.exists(file_path_train):\n",
    "            file_path = file_path_train\n",
    "            tfrecord_subdir = \"train\"\n",
    "        elif os.path.exists(file_path_test):\n",
    "            file_path = file_path_test\n",
    "            tfrecord_subdir = \"test\"\n",
    "        else:\n",
    "            print(f\"File not found in both train and test directories: {category, patient_id, modified_file_name}\")\n",
    "            continue\n",
    "\n",
    "        # Resize the image\n",
    "        image = resize_image(file_path)\n",
    "\n",
    "        # Collect all bounding boxes and labels for this image\n",
    "        bounding_boxes_list = []\n",
    "        labels = []\n",
    "        for _, row in group.iterrows():\n",
    "            if pd.notna(row['label_coordinates']):\n",
    "                bounding_box_list = row['label_coordinates'].split('|')\n",
    "                bounding_box = extract_bounding_box_coordinates(bounding_box_list)\n",
    "                bounding_box = adjust_and_scale_bounding_box(bounding_box, (2964, 2964))\n",
    "                bounding_boxes_list.append(bounding_box)\n",
    "                labels.append(1)  # Label is 1 for \"mass\"\n",
    "\n",
    "        # Define the output path for the TFRecord\n",
    "        tfrecord_dir = os.path.join(tfrecord_root, tfrecord_subdir, category, str(patient_id))\n",
    "        tfrecord_path = os.path.join(tfrecord_dir, f\"{os.path.splitext(modified_file_name)[0]}.tfrecord\")\n",
    "\n",
    "        # Create the directory if it does not exist\n",
    "        os.makedirs(tfrecord_dir, exist_ok=True)\n",
    "\n",
    "        # Check if the TFRecord file already exists\n",
    "        if os.path.exists(tfrecord_path):\n",
    "            print(f\"TFRecord already exists: {tfrecord_path}\")\n",
    "            continue\n",
    "\n",
    "        # Create a TFRecord for the image\n",
    "        create_tfrecord(image, labels, bounding_boxes_list, tfrecord_path)"
   ],
   "id": "f75720c6c02a47f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# convert_images_to_tfrecords(<hidden_path>, <hidden_path>, <hidden_path>)",
   "id": "cb1e518aa98a70c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## Checks before model training",
   "id": "7fee348ca93b6e24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def print_tfrecord_contents(tfrecord_file):\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "\n",
    "    feature_description = {\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string, default_value='jpeg'),\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    parsed_dataset = raw_dataset.map(_parse_function)\n",
    "\n",
    "    for parsed_record in parsed_dataset.take(1):\n",
    "        try:\n",
    "            image_encoded = parsed_record['image/encoded'].numpy()\n",
    "            filename = parsed_record['image/filename'].numpy().decode('utf-8')\n",
    "            image_format = parsed_record['image/format'].numpy().decode('utf-8')\n",
    "            height = parsed_record['image/height'].numpy()\n",
    "            width = parsed_record['image/width'].numpy()\n",
    "            xmin = tf.sparse.to_dense(parsed_record['image/object/bbox/xmin'], default_value=0).numpy()\n",
    "            xmax = tf.sparse.to_dense(parsed_record['image/object/bbox/xmax'], default_value=0).numpy()\n",
    "            ymin = tf.sparse.to_dense(parsed_record['image/object/bbox/ymin'], default_value=0).numpy()\n",
    "            ymax = tf.sparse.to_dense(parsed_record['image/object/bbox/ymax'], default_value=0).numpy()\n",
    "            labels = tf.sparse.to_dense(parsed_record['image/object/class/label'], default_value=-1).numpy()\n",
    "\n",
    "            print(f\"Filename: {filename}\")\n",
    "            print(f\"Image Format: {image_format}\")\n",
    "            print(f\"Height: {height}, Width: {width}\")\n",
    "            print(f\"Bounding Boxes (ymin, xmin, ymax, xmax):\")\n",
    "            for i in range(len(xmin)):\n",
    "                print(f\"  Box {i + 1}: ({ymin[i]}, {xmin[i]}, {ymax[i]}, {xmax[i]})\")\n",
    "            print(f\"Labels: {labels}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record: {e}\")"
   ],
   "id": "20eb494fc5f0de1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_bounding_boxes(image_path, bounding_boxes):\n",
    "    \"\"\"\n",
    "    Visualizes bounding boxes on the image.\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): The path to the image file.\n",
    "    bounding_boxes (list): List of bounding boxes with each box in the format [ymin, xmin, ymax, xmax].\n",
    "    \"\"\"\n",
    "    # Read and decode the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    image_np = image.numpy()\n",
    "    height, width, _ = image_np.shape\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    # Iterate over bounding boxes and add them to the plot\n",
    "    for box in bounding_boxes:\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        \n",
    "        # Convert normalized coordinates to pixel values\n",
    "        xmin_pixel = xmin * width\n",
    "        xmax_pixel = xmax * width\n",
    "        ymin_pixel = ymin * height\n",
    "        ymax_pixel = ymax * height\n",
    "        print(\"ymin:\",ymin_pixel,\"xmin:\",xmin_pixel,\"ymax:\",ymax_pixel,\"xmax:\",xmax_pixel)\n",
    "\n",
    "        box_width = xmax_pixel - xmin_pixel\n",
    "        box_height = ymax_pixel - ymin_pixel\n",
    "        rect = patches.Rectangle((xmin_pixel, ymin_pixel), box_width, box_height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ],
   "id": "ce79d9503873ea99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bounding_boxes = [[0.273479163646698, 0.6788346767425537, 0.3397921323776245, 0.7628311514854431],\n",
    "                  [0.42235761880874634, 0.6903159618377686, 0.46089544892311096, 0.7322542071342468]\n",
    "                  ]\n",
    "\n",
    "# visualize_bounding_boxes(<hidden_path>, bounding_boxes)"
   ],
   "id": "d228ab580be3f370",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Debugging",
   "id": "c765797b635563cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, feature_description)\n",
    "\n",
    "def inspect_tfrecords(tfrecord_path):\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    for raw_record in raw_dataset:\n",
    "        example = parse_tfrecord_fn(raw_record)\n",
    "        labels = tf.sparse.to_dense(example['image/object/class/label']).numpy()\n",
    "        xmin = tf.sparse.to_dense(example['image/object/bbox/xmin']).numpy()\n",
    "        xmax = tf.sparse.to_dense(example['image/object/bbox/xmax']).numpy()\n",
    "        ymin = tf.sparse.to_dense(example['image/object/bbox/ymin']).numpy()\n",
    "        ymax = tf.sparse.to_dense(example['image/object/bbox/ymax']).numpy()\n",
    "        \n",
    "        print(f\"Labels: {labels}\")\n",
    "        print(f\"Bounding boxes: xmin={xmin}, xmax={xmax}, ymin={ymin}, ymax={ymax}\")\n",
    "\n",
    "# Example usage\n",
    "def inspect_tfrecords_in_folder(folder_path):\n",
    "    tfrecord_files = glob.glob(os.path.join(folder_path, '*.tfrecord'))\n",
    "    for tfrecord_file in tfrecord_files:\n",
    "        print(f\"Inspecting {tfrecord_file}\")\n",
    "        inspect_tfrecords(tfrecord_file)\n",
    "\n",
    "# folder_path = <hidden_path>\n",
    "# inspect_tfrecords_in_folder(folder_path)"
   ],
   "id": "3de0895740272072",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Train the model",
   "id": "1c79c7c42a210cd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training is done from the terminal\n",
    "# python model_main_tf2.py --model_dir=models/my_efficientdet_d2_1 --pipeline_config_path=models/my_efficientdet_d2_1/pipeline.config"
   ],
   "id": "30e4dcbc17e78c29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Monitoring the training process\n",
    "# tensorboard --logdir=models/my_efficientdet_d0_3"
   ],
   "id": "b5bafd1ce316f5f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Export the Model",
   "id": "28f7aa9c65224be2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Export the model from the terminal\n",
    "# python .\\exporter_main_v2.py --input_type image_tensor --pipeline_config_path .\\models\\my_efficientdet_d0_5\\pipeline.config --trained_checkpoint_dir .\\models\\my_efficientdet_d0_5\\ --output_directory .\\exported_models\\my_model_5"
   ],
   "id": "c36c054173fd55d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference",
   "id": "414dca700a4962cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "# PATH_TO_SAVED_MODEL = <hidden_path>\n",
    "# PATH_TO_LABELS = <hidden_path>\n",
    "# IMAGE_PATH = <hidden_path>\n",
    "\n",
    "# Load the model\n",
    "print('Loading model...', end='')\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "print('Done!')\n",
    "\n",
    "# Load label map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "\n",
    "# Function to load an image\n",
    "def load_image_into_numpy_array(path):\n",
    "    return np.array(cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Function to run inference\n",
    "def run_inference(image_path):\n",
    "    image_np = load_image_into_numpy_array(image_path)\n",
    "    input_tensor = tf.convert_to_tensor(image_np)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_detections,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'],\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=200,\n",
    "        min_score_thresh=.30,\n",
    "        agnostic_mode=False)\n",
    "\n",
    "    return image_np_with_detections\n",
    "\n",
    "# Run inference and save the results\n",
    "result_image = run_inference(IMAGE_PATH)\n",
    "\n",
    "# output_path = <hidden_path>\n",
    "cv2.imwrite(output_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "print(f\"Result saved to {output_path}\")"
   ],
   "id": "b527604be84403de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate the Model "
   ],
   "id": "ccbc23b413b715a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation is done from the terminal\n",
    "# python model_main_tf2.py --model_dir=models/my_efficientdet_d0_5 --pipeline_config_path=models/my_efficientdet_d0_5/pipeline.config --checkpoint_dir=models/my_efficientdet_d0_5"
   ],
   "id": "c354a9f41a5c813",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
